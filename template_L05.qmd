---
title: "L05 Fitting Models"
subtitle: "Foundations of Data Science with R (STAT 359)"
author: "YOUR NAME"

format:
  html:
    toc: true
    embed-resources: true
    code-fold: false
    link-external-newwindow: true

execute:
  warning: false
  
from: markdown+emoji 
---

:::{.callout-important}
## When completing your lab write-up

Students must work in an R project connected to a GitHub repository for each lab. The repository should be well organized and it should have all relevant files. Within the project/repo there should be

- an appropriately named qmd file and 
- the associated rendered html file (see canvas for naming convention);
- there should be multiple R scripts (appropriately named and ordered) completing the work in the labs;
- students should create/update README files in GitHub accordingly;

Data processing and model fitting, especially model fitting, can take significant computational time. Re-running time consuming processes when rendering a document is extremely inefficient and must be avoided. 

This means students should practice writing these processes in scripts, saving results, and then loading them correctly when needed in their lab write-ups. It sometimes will be necessary to display code (show it, but don't run it) or even hide some code chunks when providing answers in the lab write-up. 

Remember to **make this document your own!** Meaning you should play with the layout and think about removing unnecessary sections or items (like this callout box block). Conversely you may end up adding sections or items. Make sure all of your solutions are clearly identified and communicated. 
:::

::: {.callout-tip icon=false}

## Github Repo Link

To link to your github **repo**sitory, appropriately edit the example link below. Meaning replace `https://your-github-repo-url` with your github repo url. Suggest verifying the link works before submitting.

[https://your-github-repo-url](https://your-github-repo-url)

:::

## Overview

The goal of this lab is to practice using `parsnip` to define and fit models using its standardized interface with a variety of models.

This lab accompanies [4. The Ames housing data](https://www.tmwr.org/ames.html){target="_blank"}, [5. Spending our data](https://www.tmwr.org/splitting.html){target="_blank"}, and [6. Fitting models with parsnip](https://www.tmwr.org/models.html){target="_blank"} from [Tidy Modeling with R](https://www.tmwr.org/){target="_blank"}.

::: {.callout-note collapse=true}

## Setting a Seed

Now that we are performing steps that involve randomness (for example data splitting) it is best practice to set a seed for the pseudo random algorithms. 

**Why?** Because it ensures our random steps are reproducible which has all kinds of practical benefits. Kind of mind blowing to replicate things that are supposed to be random! 

Students should set the seed directly before any random process and make a comment/note at the top of any R script that alerts potential users that a random process is being used.
:::


## Load Packages and Results

Throughout this lab there are several steps that are run in scripts and should NOT be run within this document. It is good organization to load only the needed results at the top of your document.

```{r}


```


## Exercises

### Exercise 1

We will be using the `kc_house_data.csv` dataset found in the `\data` directory. The data set contains 21,613 house sale prices (`price`) and other information for homes sold between May 2014 and May 2015 in King County, WA. Take a moment to read the variable definitions in `kc_house_data_codebook.txt`.

Read the dataset into *R* as a tibble. Take a moment to read the variable definitions in `kc_house_data_codebook.txt`.

::: {.callout-note icon="false"}
## Prediction goal

Our goal is to predict the sale prices (`price`) in King County, WA.

:::

#### Task 1

Our first step is to do a quick inspection of the outcome/target variable, **AND ONLY** the outcome/target variable, `price`.

Are there issues with the distribution of `price`? If so, perform an appropriate transformation. This should be done in the `exercise_1/1_initial_setup.R` script, but remember to provide display code for graders.  

::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE

:::

#### Task 2

Split the data into a training set and a testing set. Use stratified sampling. You should decide on appropriate percentages for splitting the data. This should be done in the `exercise_1/1_initial_setup.R` script. Save the training and testing data to a `results` folder.

*Remember to set a seed so work is reproducible.*

::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE: Display code for splitting the data is required for the graders to verify this was done correctly. "Display code" should NOT be evaluated within this document.

:::

Why are we splitting the data into training data and testing data?

::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE

:::

::: {.callout-caution}

As you work through this lab, pay close attention to which dataset you are using: training or testing.

:::


#### Task 3

Using the **training** data, create a simple recipe. We want to predict the outcome/target variable with `waterfront`, `sqft_living`, `yr_built`, and `bedrooms`.

This recipe should be built in `exercise_1/2_recipes.R`, but remember to provide display code for graders. Save the recipe to a `results` folder. We will learn more about building recipes next week but for now we will ignore the process of feature engineering.


::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE --- display code is needed to verify recipe was defined correctly

:::


#### Task 4

Define your linear regression model using the `"lm"` engine. in `exercise_1/3_fit_lm.R`.

::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE --- display code is needed to verify model was defined correctly

:::

#### Task 5

Define/create a workflow called `lm_wflow` for training a linear regression model in `exercise_1/3_fit_lm.R`. Add your specified model and the appropriate recipe. 

::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE --- display code is needed to verify workflow was defined correctly

:::


#### Task 6

Use `fit()` to train your workflow. Save these results. 

This work should be completed in `exercise_1/3_fit_lm.R`, but remember to provide display code for graders.  

::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE --- display code is needed. Reminder --- never run model fitting in a qmd.

:::


#### Task 7

Now you want to assess your model's performance on several metrics in the `exercise_1/4_model_analysis.R`. To do this, use the `yardstick` package:

1.  Create a metric set that includes *R^2^*, RMSE (root mean squared error), and MAE (mean absolute error).
2.  Use `predict()` and `bind_cols()` to create a tibble of your model's predicted values for the testing data along with the actual observed values (these are needed to assess your model's performance).
3.  Finally, apply your metric set to the tibble, report the results, and provide an interpretation of each of the values --- MAE and RMSE are interpreted similarly while *R^2^* has a different interpretation.


::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE --- this code should run and be evaluated.

:::

#### Task 8

Create a plot of the predicted values versus the true values.


::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE --- this code should run and be evaluated.

:::

#### Task 9

Repeat Task 7 and 8 but this time report the predictions on the original scale.


::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE --- this code should run and be evaluated.

:::

### Exercise 2

::: {.callout-important}

Unlike Exercise 1, there are no explicit instruction on where to place your code and no explicit instructions about what to display to graders. 

Students are expected to figure this out.

Students will need to organize their work and present sufficient evidence for graders --- use exercise 1 for guidance. Some initial R scripts template are provided to get students started.

:::


For this exercise, we will be working with part of a [Kaggle data set](https://www.kaggle.com/c/titanic/overview) that was the subject of a machine learning competition and is often used for practicing ML models.


::: {.callout-note icon="false"}
## Prediction goal

The goal is classification; specifically, to predict which passengers would survive the [Titanic shipwreck](https://en.wikipedia.org/wiki/Titanic).
:::

#### Task 1

Load the data from `data/titanic.csv` into *R* and familiarize yourself with the variables by reviewing the codebook (`data/titanic_codebook.csv`).

Notice that `survived`, `pclass`, and `sex` should be changed to factors. When changing `survived` to a factor, you should reorder the factor so that `"Yes"` is the first level.


::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE

:::

#### Task 2

Using the full data set, explore/describe the distribution of the outcome variable `survived`. Is the outcome variable balanced or imbalanced?

**Only do this for the target variable.**


::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE

:::

#### Task 3

Split the data! Use stratified sampling. You should choose the proportions to split the data into.


::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE

:::

Why is it a good idea to use stratified sampling for this data?

::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE

:::

#### Task 4

Create and store a simple recipe setting `survived` as the outcome and using the following predictors: ticket class, sex, number of siblings or spouses aboard, number of parents or children aboard, and passenger fare.


::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE

:::

#### Task 5

Define your logistic regression model using the `"glm"` engine.


::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE

:::

#### Task 6

Create a workflow for fitting a **logistic regression** model. Add your specified model and the appropriate recipe.


::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE

:::

#### Task 7

Use `fit()` to train your workflow. 

::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE

:::

#### Task 8

Now you want to assess your model's performance.

1) Use `predict()` and `bind_cols()` to generate **class** predictions using your testing data. Then use the **accuracy** metric to assess the performance.

::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE

:::

2) Create a confusion matrix using the testing data. Explain what this is in your own words. Interpret the numbers in each category.

::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE

:::

#### Task 9

Now we will assess the model performance using a different metric.

1) Use `predict()` and `bind_cols()` to generate **probability** predictions using your testing data. Then use the **roc_auc** metric to assess the performance.

::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE

:::

2) Use `roc_curve()` and `autoplot()` to create a receiver operating characteristic (ROC) curve. Interpret the AUC for your model.

Note: The area under the ROC curve is a measure of how well the model predictions are able to separate the data being tested into classes/groups. [(See here for a more detailed explanation)](http://gim.unmc.edu/dxtests/roc3.htm).

::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE

:::
